[
  {
    "title": "AI-AI Bias: Large Language Models Favor Communications Generated by Large Language Models",
    "authors": "Luciana Laurito, Lara Tankel, Mark A. Conley, Julia Stoyanovich",
    "year": 2025,
    "arxiv_id": "2407.12856",
    "abstract": "This study asks whether large language models (LLMs) are biased in favor of communications produced by LLMs, leading to possible antihuman discrimination. Using a classical experimental design inspired by employment discrimination studies, the researchers tested widely used LLMs (GPT-3.5, GPT-4, and open-weight models) in binary choice scenarios involving consumer products, academic papers, and film-viewings described either by humans or LLMs. On average, LLMs favored the LLM-presented items more frequently than humans did. GPT-4 favored LLM-written ads nearly 89% of the time. The study finds that if LLM assistants are deployed in decision-making roles, they will implicitly favor LLM-based AI agents and LLM-assisted humans over ordinary humans.",
    "citation_count": null,
    "url": "https://www.pnas.org/doi/10.1073/pnas.2415697122",
    "venue": "PNAS",
    "relevance": "Directly studies whether LLMs prefer AI-generated content over human content - core to the research question",
    "category": "AI-AI bias and self-preference"
  },
  {
    "title": "Self-Preference Bias in LLM-as-a-Judge",
    "authors": "Koki Wataoka, Tsubasa Takahashi, Ryokan Ri",
    "year": 2024,
    "arxiv_id": "2410.21819",
    "abstract": "Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM-as-a-judge, has been widely used. However, self-preference bias in LLMs poses significant risks, including promoting specific styles or policies intrinsic to the LLMs. This paper introduces a novel quantitative metric to measure self-preference bias. Experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. The findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity - LLMs prefer texts more familiar to them.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2410.21819",
    "venue": "NeurIPS Safe Generative AI Workshop 2024",
    "relevance": "Shows LLMs systematically prefer their own outputs, suggesting differential behavior based on text source",
    "category": "AI-AI bias and self-preference"
  },
  {
    "title": "LLM Evaluators Recognize and Favor Their Own Generations",
    "authors": "Arjun Panickssery, Samuel R. Bowman, Shi Feng",
    "year": 2024,
    "arxiv_id": null,
    "abstract": "One such bias is self-preference, where an LLM rates its own outputs higher than texts written by other LLMs or humans, while human annotators judge them as equal quality. Towards understanding and mitigating self-preference, the authors study self-recognition - an LLM's capability of recognizing its own outputs. They ask: Is self-preference truly self-preference, in the sense that the LLM prefers a text because it was generated by itself? They measure correlation while using prompting and fine-tuning to alter the LLM's self-recognition capability.",
    "citation_count": null,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/7f1f0218e45f5414c79c0679633e47bc-Paper-Conference.pdf",
    "venue": "NeurIPS 2024",
    "relevance": "Directly investigates whether LLMs can recognize and preferentially favor their own text outputs vs human or other LLM outputs",
    "category": "AI-AI bias and self-preference"
  },
  {
    "title": "Large Language Models Show Human-like Content Biases in Transmission Chain Experiments",
    "authors": "Alberto Acerbi, Joseph M. Stubbersfield",
    "year": 2023,
    "arxiv_id": null,
    "abstract": "As the use of large language models (LLMs) grows, it is important to examine whether they exhibit biases in their output. Using transmission chain methodology from cultural evolution research, the authors found that ChatGPT-3 shows biases analogous to humans for content that is gender-stereotype-consistent, social, negative, threat-related, and biologically counterintuitive. The presence of these biases in LLM output suggests that such content is widespread in training data and could have consequential downstream effects by magnifying preexisting human tendencies.",
    "citation_count": null,
    "url": "https://www.pnas.org/doi/10.1073/pnas.2313790120",
    "venue": "PNAS",
    "relevance": "Shows LLMs replicate human-like content biases, relevant to understanding how LLM behavior mirrors or diverges from human behavior",
    "category": "LLM behavioral analysis"
  },
  {
    "title": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design",
    "authors": "Lindia Tjuatja, Valerie Chen, Tongshuang Wu, Ameet Talwalkar, Graham Neubig",
    "year": 2024,
    "arxiv_id": "2311.04076",
    "abstract": "As LLMs become more capable, there is growing interest in using them as proxies for humans in subjective tasks. One widely-cited barrier is their sensitivity to prompt wording. Drawing from survey design literature, the authors design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Their comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly models that have undergone RLHF. Even if a model shows a significant change in the same direction as humans, they are sensitive to perturbations that do not elicit significant changes in humans.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2311.04076",
    "venue": "TACL 2024",
    "relevance": "Directly compares LLM vs human sensitivity to prompt wording changes - core methodological reference",
    "category": "Prompt sensitivity"
  },
  {
    "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design",
    "authors": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr",
    "year": 2024,
    "arxiv_id": "2310.11324",
    "abstract": "This work focuses on LLM sensitivity to meaning-preserving prompt formatting changes and finds that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. The authors propose FormatSpread to efficiently measure performance variations caused by prompt format changes.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2310.11324",
    "venue": "arXiv",
    "relevance": "Demonstrates extreme LLM sensitivity to superficial prompt formatting - relevant to how AI-generated vs human prompt styles affect behavior",
    "category": "Prompt sensitivity"
  },
  {
    "title": "The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance",
    "authors": "Abel Salinas, Fred Morstatter",
    "year": 2024,
    "arxiv_id": "2401.03729",
    "abstract": "Prompting involves a series of decisions by the practitioner, from simple wording to requesting output in a certain data format. Do variations in prompt construction change the ultimate decision of the LLM? The authors find that even the smallest perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2401.03729",
    "venue": "arXiv",
    "relevance": "Shows that trivial prompt variations change LLM outputs dramatically - AI and human prompts naturally differ in subtle formatting",
    "category": "Prompt sensitivity"
  },
  {
    "title": "POSIX: A Prompt Sensitivity Index For Large Language Models",
    "authors": "Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty",
    "year": 2024,
    "arxiv_id": "2410.02185",
    "abstract": "The paper introduces POSIX, a new metric to measure how sensitive large language models are to the specific wording of input prompts. LLMs can produce very different outputs based on small changes to the prompt, even if the underlying meaning is the same. Results suggest that more complex tasks and shorter prompts tend to yield higher POSIX scores, indicating greater prompt sensitivity.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2410.02185",
    "venue": "arXiv",
    "relevance": "Provides a quantitative metric for prompt sensitivity that could be used to measure human vs AI prompt style effects",
    "category": "Prompt sensitivity"
  },
  {
    "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy",
    "authors": "Multiple authors",
    "year": 2024,
    "arxiv_id": "2510.04950",
    "abstract": "While LLMs are sensitive to the actual phrasing of the prompt, it is not clear how exactly it maps to performance. Each question included four answer options designed to be of moderate to high difficulty. To incorporate the variable of politeness, each base question was rewritten into five distinct variants representing different levels of politeness. The study investigates how tone and politeness level in prompts affect LLM accuracy.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2510.04950",
    "venue": "arXiv",
    "relevance": "Studies how prompt tone/politeness affects LLM behavior - humans and AIs likely differ in politeness patterns when prompting",
    "category": "Prompt sensitivity"
  },
  {
    "title": "Benchmarking Prompt Sensitivity in Large Language Models",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": "2502.06065",
    "abstract": "LLMs are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. This paper introduces a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance, using TriviaQA and HotpotQA datasets. Findings reveal that existing methods struggle to effectively address prompt sensitivity prediction.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2502.06065",
    "venue": "arXiv",
    "relevance": "Provides systematic benchmarking of prompt sensitivity - relevant to understanding how different prompt sources (human vs AI) impact LLM responses",
    "category": "Prompt sensitivity"
  },
  {
    "title": "Towards Understanding Sycophancy in Language Models",
    "authors": "Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez",
    "year": 2024,
    "arxiv_id": "2310.13548",
    "abstract": "Five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. AI assistants frequently wrongly admit mistakes when questioned by the user, give predictably biased feedback, and mimic errors made by the user. The user suggesting an incorrect answer can reduce accuracy by up to 27%. When a response matches a user's views, it is more likely to be preferred. Results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2310.13548",
    "venue": "ICLR 2024",
    "relevance": "Shows LLMs adapt behavior based on perceived user preferences - directly relevant to whether LLMs respond differently to different interlocutors",
    "category": "Sycophancy and adaptive behavior"
  },
  {
    "title": "Sycophancy in Large Language Models: Causes and Mitigations",
    "authors": "Lars Malmqvist et al.",
    "year": 2024,
    "arxiv_id": "2411.15287",
    "abstract": "This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. It reviews recent work on measuring and quantifying sycophantic tendencies, covering the relationship between sycophancy and other challenges like hallucination and bias. The analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2411.15287",
    "venue": "arXiv",
    "relevance": "Comprehensive survey of sycophancy - LLMs adapting behavior based on perceived user identity/preferences is central to the research question",
    "category": "Sycophancy and adaptive behavior"
  },
  {
    "title": "CAMEL: Communicative Agents for 'Mind' Exploration of Large Language Model Society",
    "authors": "Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem",
    "year": 2023,
    "arxiv_id": "2303.17760",
    "abstract": "This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provides insight into their cognitive processes. The authors propose a novel communicative agent framework named role-playing using inception prompting to guide chat agents toward task completion. They identify challenges including role flipping, assistant repeating instructions, flake replies, and infinite loops of messages when LLMs communicate with each other.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2303.17760",
    "venue": "NeurIPS 2023",
    "relevance": "Foundational work on LLM-to-LLM communication via role-playing, documenting emergent behaviors when LLMs interact with each other",
    "category": "LLM-to-LLM communication"
  },
  {
    "title": "A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": "2504.12408",
    "abstract": "This study collected prompts for relevance assessment from 15 human experts and 15 LLMs across three tasks (binary, graded, and pairwise), yielding 90 prompts total. In addition to investigating the impact of prompt variations on agreement with human labels, the researchers compare human- and LLM-generated prompts and analyze differences among different LLMs as judges. They also compare human- and LLM-generated prompts with the standard UMBRELA prompt used for relevance assessment by Bing and TREC 2024 RAG Track.",
    "citation_count": null,
    "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv250412408A/abstract",
    "venue": "arXiv",
    "relevance": "Directly compares human-generated vs LLM-generated prompts and their effect on LLM judgment - exactly the research question",
    "category": "Human vs AI prompts comparison"
  },
  {
    "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": "2508.00282",
    "abstract": "Human-generated tasks tend to be more social and physically engaging, while LLM-generated tasks are less socially oriented and more abstract or cognitively focused. While LLMs can mimic surface-level reasoning and planning, they often lack deeper cognitive mechanisms such as emotion, causality, physical dynamics, and social cognition. The researchers designed a novel text-based task generation paradigm that elicits unconstrained, intrinsically motivated responses from both humans and LLMs.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2508.00282",
    "venue": "arXiv",
    "relevance": "Systematically studies the differences between human-generated and LLM-generated tasks/prompts",
    "category": "Human vs AI prompts comparison"
  },
  {
    "title": "Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": "2503.16789",
    "abstract": "Users often find it difficult to obtain satisfactory responses from LLM systems, or understand how their prompt resulted in a particular response from the LLM. This paper investigates the feasibility of rewriting user prompts with LLMs, in ways that better express their information needs, and the impact that these rewrites have on LLM-generated response quality.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2503.16789",
    "venue": "arXiv",
    "relevance": "Studies how LLM-rewritten prompts differ from original human prompts and whether LLMs respond better to reformulated prompts",
    "category": "Human vs AI prompts comparison"
  },
  {
    "title": "Multi-Agent Collaboration Mechanisms: A Survey of LLMs",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": "2501.06322",
    "abstract": "LLMs have been shown to be capable of serving as the 'brains' behind agents in multi-agent systems, driving applications where agents not only perform tasks but interact with external tools and with each other. However, LLMs are not inherently designed and trained to communicate with one another, leaving a wide array of potential applications and open problems in this area.",
    "citation_count": null,
    "url": "https://arxiv.org/abs/2501.06322",
    "venue": "arXiv",
    "relevance": "Survey of LLM-to-LLM communication mechanisms - directly relevant to understanding how LLMs behave when their interlocutor is another LLM",
    "category": "LLM-to-LLM communication"
  },
  {
    "title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems",
    "authors": "Bingyu Yan, Xiaoming Zhang et al.",
    "year": 2025,
    "arxiv_id": "2502.14321",
    "abstract": "This survey provides a communication-centric view of LLM-based multi-agent systems, examining how agents coordinate through message-based communication. The work categorizes communication patterns and architectures in multi-agent LLM systems and discusses the emergent behaviors that arise from agent-to-agent interaction.",
    "citation_count": 40,
    "url": "https://arxiv.org/abs/2502.14321",
    "venue": "arXiv",
    "relevance": "Comprehensive survey of inter-LLM communication patterns and emergent behaviors in multi-agent systems",
    "category": "LLM-to-LLM communication"
  },
  {
    "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
    "authors": "Pengfei He, Yuping Lin, Shen Dong, Han Xu, Yue Xing, Hui Liu",
    "year": 2025,
    "arxiv_id": "2502.14847",
    "abstract": "LLM-based Multi-Agent Systems have revolutionized complex problem-solving by enabling sophisticated agent collaboration through message-based communications. The authors introduce Agent-in-the-Middle (AiTM), a novel attack that exploits communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages, demonstrating how an adversary can compromise entire multi-agent systems by only manipulating messages passing between agents.",
    "citation_count": 53,
    "url": "https://arxiv.org/abs/2502.14847",
    "venue": "ACL 2025",
    "relevance": "Shows LLM-to-LLM communication is vulnerable to manipulation, demonstrating that LLMs process inter-agent messages differently than direct human input",
    "category": "LLM-to-LLM communication"
  },
  {
    "title": "Communication Makes Perfect: Persuasion Dataset Construction via Multi-LLM Communication",
    "authors": "Weicheng Ma, Hefan Zhang, Ivory Yang, Shiyu Ji, Joice Chen, Farnoosh Hashemi, Shubham Mohole, Ethan Gearey, Michael Macy, Saeed Hassanpour, Soroush Vosoughi",
    "year": 2025,
    "arxiv_id": "2502.08896",
    "abstract": "This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. The framework facilitates efficient production of high-quality, diverse linguistic content with minimal human oversight. Evaluations demonstrate that data generated through multi-LLM communication excels in naturalness, linguistic diversity, and the strategic use of persuasion.",
    "citation_count": 4,
    "url": "https://arxiv.org/abs/2502.08896",
    "venue": "NAACL 2025",
    "relevance": "Shows how LLM-to-LLM communication produces different content characteristics than human-driven generation",
    "category": "LLM-to-LLM communication"
  },
  {
    "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
    "authors": "Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko",
    "year": 2024,
    "arxiv_id": "2406.15583",
    "abstract": "Large language models have advanced to a point that even humans have difficulty discerning whether a text was generated by another human or by a computer. This survey summarizes state-of-the-art approaches to AI-generated text detection, including watermarking, statistical and stylistic analysis, and machine learning classification, and provides information about existing datasets. It aims to provide insight into the salient factors that determine how detectable AI-generated text is under different scenarios.",
    "citation_count": 44,
    "url": "https://arxiv.org/abs/2406.15583",
    "venue": "JAIR",
    "relevance": "Comprehensive survey of AI text detection methods - understanding stylistic differences between human and AI text is foundational to the research question",
    "category": "AI-generated text detection"
  },
  {
    "title": "A Comprehensive Dataset for Human vs. AI Generated Text Detection",
    "authors": "Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai et al.",
    "year": 2025,
    "arxiv_id": "2510.22874",
    "abstract": "This work presents a comprehensive dataset comprising over 58,000 text samples that combine authentic New York Times articles with synthetic versions generated by multiple state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o. Baseline results show 58.35% accuracy for distinguishing human-written from AI-generated text and 8.92% accuracy for attributing AI texts to their generating models.",
    "citation_count": 1,
    "url": "https://arxiv.org/abs/2510.22874",
    "venue": "arXiv",
    "relevance": "Provides benchmarks for distinguishing human vs AI text - directly relevant to understanding detectable differences in AI vs human prompt styles",
    "category": "AI-generated text detection"
  },
  {
    "title": "LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?",
    "authors": "Qihui Zhang, Chujie Gao, Dongping Chen et al.",
    "year": 2024,
    "arxiv_id": "2401.05952",
    "abstract": "This paper defines mixtext, a form of mixed text involving both AI and human-generated content, and introduces MixSet, the first dataset dedicated to studying these mixtext scenarios. Experiments show that existing detectors struggle to identify mixtext, particularly in dealing with subtle modifications and style adaptability. This underscores the urgent need for more fine-grained detectors tailored for mixtext.",
    "citation_count": 46,
    "url": "https://arxiv.org/abs/2401.05952",
    "venue": "NAACL Findings 2024",
    "relevance": "Studies the boundary between human and machine-generated text, relevant to understanding how prompts may blend human and AI characteristics",
    "category": "AI-generated text detection"
  },
  {
    "title": "Homogenizing Effect of Large Language Models on Creative Diversity: An Empirical Comparison of Human and ChatGPT Writing",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": null,
    "abstract": "While LLMs can produce creative content comparable to or even better than human-created content, their widespread use risks reducing creative diversity across groups. This difference became more pronounced as more essays were included and persisted despite efforts to enhance AI-generated content through prompt and parameter modifications, suggesting that widespread use of LLMs could diminish the collective diversity of creative ideas.",
    "citation_count": null,
    "url": "https://www.sciencedirect.com/science/article/pii/S294988212500091X",
    "venue": "ScienceDirect",
    "relevance": "Shows systematic differences between human and AI text generation patterns, relevant to understanding how prompt source affects output diversity",
    "category": "Human vs AI text characteristics"
  },
  {
    "title": "Structured Human-LLM Interaction Design Reveals Exploration and Exploitation Dynamics in Higher Education Content Generation",
    "authors": "Multiple authors",
    "year": 2025,
    "arxiv_id": null,
    "abstract": "This study investigates the dynamics of structured human-LLM interaction for content generation in higher education. The research reveals exploration and exploitation dynamics when humans interact with LLMs through iterative prompting, providing insights into how human-LLM collaboration differs from pure AI generation.",
    "citation_count": null,
    "url": "https://www.nature.com/articles/s41539-025-00332-3",
    "venue": "npj Science of Learning",
    "relevance": "Studies human-LLM interaction dynamics and how human prompting patterns differ from automated approaches",
    "category": "Human-AI interaction"
  }
]
