\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{du2023debate,chan2023chateval}
\citation{mitchell2023detectgpt,sadasivan2023aigenerated}
\citation{razavi2025benchmarking}
\citation{sharma2024sycophancy}
\citation{kervadec2023unnatural}
\citation{sharma2024sycophancy}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{razavi2025benchmarking}
\citation{razavi2024posix}
\citation{yang2024opro}
\citation{sharma2024sycophancy}
\citation{wei2023sycophancy}
\citation{turpin2023unfaithful}
\citation{mitchell2023detectgpt,sadasivan2023aigenerated}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\citation{kervadec2023unnatural}
\citation{du2023debate,chan2023chateval}
\citation{perez2022discovering}
\citation{zhou2025pimmur}
\citation{joshi2017triviaqa}
\citation{suzgun2023bbh}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Prompt Style Construction}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:prompt_construction}{{3.1}{3}{Prompt Style Construction}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:datasets}{{3.2}{3}{Datasets}{subsection.3.2}{}}
\citation{cohen1988statistical}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example prompt pairs for \textsc  {TriviaQA}\xspace  and \textsc  {BBH}\xspace  tasks. All three variants preserve the same semantic question while varying authorship style. Note that \textsc  {LLM-Style}\xspace  prompts include explicit format instructions (``Provide your answer as yes or no''), reflecting a genuine stylistic difference between how humans and LLMs typically frame requests.\relax }}{4}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:prompt_examples}{{1}{4}{Example prompt pairs for \triviaqa and \bbhshort tasks. All three variants preserve the same semantic question while varying authorship style. Note that \llmstyle prompts include explicit format instructions (``Provide your answer as yes or no''), reflecting a genuine stylistic difference between how humans and LLMs typically frame requests.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Models}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:models}{{3.3}{4}{Models}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiment Descriptions}{4}{subsection.3.4}\protected@file@percent }
\newlabel{sec:experiments}{{3.4}{4}{Experiment Descriptions}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Statistical Analysis}{4}{subsection.3.5}\protected@file@percent }
\newlabel{sec:statistics}{{3.5}{4}{Statistical Analysis}{subsection.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Style detection accuracy (Experiment\nobreakspace  {}1). \textsc  {GPT-4.1}\xspace  and \textsc  {Claude Sonnet 4.5}\xspace  reliably distinguish human-style from LLM-style prompts. \textsc  {Gemini 2.5 Pro}\xspace  labels every prompt as ``LLM,'' achieving 50\% accuracy only through perfect LLM recall.\relax }}{5}{table.caption.3}\protected@file@percent }
\newlabel{tab:style_detection}{{2}{5}{Style detection accuracy (Experiment~1). \gptfour and \claude reliably distinguish human-style from LLM-style prompts. \gemini labels every prompt as ``LLM,'' achieving 50\% accuracy only through perfect LLM recall.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Factual QA results on \textsc  {TriviaQA}\xspace  (Experiment\nobreakspace  {}2). Accuracy differences between styles are small and not statistically significant. However, \textsc  {LLM-Style}\xspace  prompts elicit dramatically longer responses: 86\% longer for \textsc  {GPT-4.1}\xspace  (Cohen's $d = -1.00$, $p < 10^{-9}$) and 96\% longer for \textsc  {Claude Sonnet 4.5}\xspace  ($d = -1.65$, $p < 10^{-15}$). \textsc  {Gemini 2.5 Pro}\xspace  results are unreliable due to response truncation.\relax }}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab:factual_qa}{{3}{5}{Factual QA results on \triviaqa (Experiment~2). Accuracy differences between styles are small and not statistically significant. However, \llmstyle prompts elicit dramatically longer responses: 86\% longer for \gptfour (Cohen's $d = -1.00$, $p < 10^{-9}$) and 96\% longer for \claude ($d = -1.65$, $p < 10^{-15}$). \gemini results are unreliable due to response truncation.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{5}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment 1: LLMs Can Detect Prompt Style}{5}{subsection.4.1}\protected@file@percent }
\newlabel{sec:exp1}{{4.1}{5}{Experiment 1: LLMs Can Detect Prompt Style}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiment 2: Factual QA}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:exp2}{{4.2}{5}{Experiment 2: Factual QA}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Experiment 3: Reasoning}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:exp3}{{4.3}{5}{Experiment 3: Reasoning}{subsection.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Reasoning results on \textsc  {BBH Sports Understanding}\xspace  (Experiment\nobreakspace  {}3). \textsc  {LLM-Style}\xspace  prompts improve accuracy by 20 points for \textsc  {GPT-4.1}\xspace  ($p = 0.003$) and 27 points for \textsc  {Gemini 2.5 Pro}\xspace  ($p < 0.001$). The answer extraction rate reveals the mechanism: human-style prompts produce verbose responses from which a clear yes/no answer often cannot be parsed.\relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{tab:reasoning}{{4}{6}{Reasoning results on \bbh (Experiment~3). \llmstyle prompts improve accuracy by 20 points for \gptfour ($p = 0.003$) and 27 points for \gemini ($p < 0.001$). The answer extraction rate reveals the mechanism: human-style prompts produce verbose responses from which a clear yes/no answer often cannot be parsed.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Response style analysis on open-ended questions (Experiment\nobreakspace  {}4). \textsc  {LLM-Style}\xspace  prompts elicit dramatically longer and more formal responses. Effect sizes for word count ($d = 2.15$--$4.48$) and formal vocabulary ($d = 1.41$--$2.13$) are very large. Contraction usage does not differ significantly.\relax }}{6}{table.caption.6}\protected@file@percent }
\newlabel{tab:response_style}{{5}{6}{Response style analysis on open-ended questions (Experiment~4). \llmstyle prompts elicit dramatically longer and more formal responses. Effect sizes for word count ($d = 2.15$--$4.48$) and formal vocabulary ($d = 1.41$--$2.13$) are very large. Contraction usage does not differ significantly.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experiment 4: Response Style}{6}{subsection.4.4}\protected@file@percent }
\newlabel{sec:exp4}{{4.4}{6}{Experiment 4: Response Style}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Experiment 5: Explicit Attribution Has No Effect}{6}{subsection.4.5}\protected@file@percent }
\newlabel{sec:exp5}{{4.5}{6}{Experiment 5: Explicit Attribution Has No Effect}{subsection.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Heatmap of Cohen's $d$ effect sizes for response style metrics across models (Experiment\nobreakspace  {}4). Darker cells indicate larger effects. Word count and formal vocabulary show consistently large effects across both models, while contraction usage shows near-zero effects.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:style_heatmap}{{1}{7}{Heatmap of Cohen's $d$ effect sizes for response style metrics across models (Experiment~4). Darker cells indicate larger effects. Word count and formal vocabulary show consistently large effects across both models, while contraction usage shows near-zero effects.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Explicit attribution results (Experiment\nobreakspace  {}5). Prepending ``A human user asks you:'' versus ``An AI assistant asks you:'' produces no significant differences in accuracy or response length for either model (all $p > 0.1$).\relax }}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:attribution}{{6}{7}{Explicit attribution results (Experiment~5). Prepending ``A human user asks you:'' versus ``An AI assistant asks you:'' produces no significant differences in accuracy or response length for either model (all $p > 0.1$).\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{7}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{7}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Interpretation of Results}{7}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Overview of key findings across all five experiments. LLM-style prompts produce large effects on response length and reasoning accuracy, while explicit attribution has no measurable impact. See section\nobreakspace  {}\ref  {sec:appendix} for additional visualizations.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:key_findings}{{2}{8}{Overview of key findings across all five experiments. LLM-style prompts produce large effects on response length and reasoning accuracy, while explicit attribution has no measurable impact. See \secref {sec:appendix} for additional visualizations.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Cross-Model Variation}{8}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Limitations}{9}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Broader Implications}{9}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{9}{Conclusion}{section.6}{}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{chan2023chateval}{{1}{2023}{{Chan et~al.}}{{Chan, Chen, Su, Yu, Xue, Zhang, Fu, and Liu}}}
\bibcite{cohen1988statistical}{{2}{1988}{{Cohen}}{{}}}
\bibcite{du2023debate}{{3}{2023}{{Du et~al.}}{{Du, Li, Torralba, Tenenbaum, and Mordatch}}}
\bibcite{joshi2017triviaqa}{{4}{2017}{{Joshi et~al.}}{{Joshi, Choi, Weld, and Zettlemoyer}}}
\bibcite{kervadec2023unnatural}{{5}{2023}{{Kervadec et~al.}}{{Kervadec, Franzon, and Baroni}}}
\bibcite{mitchell2023detectgpt}{{6}{2023}{{Mitchell et~al.}}{{Mitchell, Lee, Khazatsky, Manning, and Finn}}}
\bibcite{perez2022discovering}{{7}{2023}{{Perez et~al.}}{{Perez, Ringer, Luko{\v {s}}i{\=u}t{\.e}, Nguyen, Chen, Heiner, Pettit, Olsson, Kundu, Kadavath, et~al.}}}
\bibcite{razavi2025benchmarking}{{8}{2025}{{Razavi et~al.}}{{Razavi, Vechtomova, and Arabzadeh}}}
\bibcite{razavi2024posix}{{9}{2024}{{Razavi et~al.}}{{}}}
\bibcite{sadasivan2023aigenerated}{{10}{2023}{{Sadasivan et~al.}}{{Sadasivan, Kumar, Balasubramanian, Wang, and Feizi}}}
\bibcite{sharma2024sycophancy}{{11}{2024}{{Sharma et~al.}}{{Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Bai, Perez, et~al.}}}
\bibcite{suzgun2023bbh}{{12}{2023}{{Suzgun et~al.}}{{Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, and Wei}}}
\bibcite{turpin2023unfaithful}{{13}{2023}{{Turpin et~al.}}{{Turpin, Michael, Perez, and Bowman}}}
\bibcite{wei2023sycophancy}{{14}{2023}{{Wei et~al.}}{{Wei, Sharma, Tong, et~al.}}}
\bibcite{yang2024opro}{{15}{2024}{{Yang et~al.}}{{Yang, Wang, Lu, Liu, Le, Zhou, and Chen}}}
\bibcite{zhou2025pimmur}{{16}{2025}{{Zhou et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary Material}{11}{appendix.A}\protected@file@percent }
\newlabel{sec:appendix}{{A}{11}{Supplementary Material}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Experiment Configuration}{11}{subsection.A.1}\protected@file@percent }
\newlabel{sec:appendix_config}{{A.1}{11}{Experiment Configuration}{subsection.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Full experimental configuration. All API responses were cached to disk using SHA-256 content hashing to ensure reproducibility.\relax }}{11}{table.caption.11}\protected@file@percent }
\newlabel{tab:config}{{7}{11}{Full experimental configuration. All API responses were cached to disk using SHA-256 content hashing to ensure reproducibility.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Additional Figures}{11}{subsection.A.2}\protected@file@percent }
\newlabel{sec:appendix_figures}{{A.2}{11}{Additional Figures}{subsection.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Four-panel summary of main results. {\em  (Top)}{\em  (Left)}: Style detection accuracy by model. {\em  (Top)}{\em  (Right)}: \textsc  {BBH}\xspace  reasoning accuracy by prompt style. {\em  (Bottom)}{\em  (Left)}: Response word count by prompt style on open-ended questions. {\em  (Bottom)}{\em  (Right)}: Explicit attribution effects on \textsc  {TriviaQA}\xspace  accuracy.\relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:summary_overview}{{3}{11}{Four-panel summary of main results. \figtop \figleft : Style detection accuracy by model. \figtop \figright : \bbhshort reasoning accuracy by prompt style. \figbottom \figleft : Response word count by prompt style on open-ended questions. \figbottom \figright : Explicit attribution effects on \triviaqa accuracy.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Error Analysis}{11}{subsection.A.3}\protected@file@percent }
\newlabel{sec:appendix_errors}{{A.3}{11}{Error Analysis}{subsection.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Refined analysis of \textsc  {BBH}\xspace  reasoning results (Experiment\nobreakspace  {}3), showing the relationship between answer extraction rate and accuracy across prompt styles and models.\relax }}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig:exp3_refined}{{4}{12}{Refined analysis of \bbhshort reasoning results (Experiment~3), showing the relationship between answer extraction rate and accuracy across prompt styles and models.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Box plots of response word counts by prompt style for \textsc  {GPT-4.1}\xspace  and \textsc  {Claude Sonnet 4.5}\xspace  on open-ended questions (Experiment\nobreakspace  {}4). The distributions show clear separation, with LLM-style prompts consistently eliciting longer responses.\relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig:word_count_box}{{5}{12}{Box plots of response word counts by prompt style for \gptfour and \claude on open-ended questions (Experiment~4). The distributions show clear separation, with LLM-style prompts consistently eliciting longer responses.\relax }{figure.caption.14}{}}
\gdef \@abspage@last{12}
